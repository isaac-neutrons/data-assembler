"""
Parquet parser for nexus-processor output files.

Reads and interprets parquet files generated by nexus-processor:
- metadata.parquet
- sample.parquet
- instrument.parquet
- users.parquet
- daslogs.parquet
"""

from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

import pyarrow.parquet as pq


@dataclass
class MetadataRecord:
    """Parsed metadata from metadata.parquet."""

    instrument_id: str
    run_number: int
    run_id: str
    title: Optional[str] = None
    start_time: Optional[str] = None
    end_time: Optional[str] = None
    duration: Optional[float] = None
    proton_charge: Optional[float] = None
    total_counts: Optional[int] = None
    experiment_identifier: Optional[str] = None  # IPTS number
    definition: Optional[str] = None
    source_file: Optional[str] = None
    source_path: Optional[str] = None
    ingestion_time: Optional[str] = None
    file_attributes: Optional[dict] = None
    entry_attributes: Optional[dict] = None


@dataclass
class SampleRecord:
    """Parsed sample info from sample.parquet."""

    instrument_id: str
    run_number: int
    run_id: str
    name: Optional[str] = None
    nature: Optional[str] = None
    chemical_formula: Optional[str] = None
    mass: Optional[float] = None
    temperature: Optional[float] = None
    additional_fields: Optional[dict] = None


@dataclass
class InstrumentRecord:
    """Parsed instrument info from instrument.parquet."""

    instrument_id: str
    run_number: int
    run_id: str
    name: Optional[str] = None
    beamline: Optional[str] = None
    instrument_xml_data: Optional[str] = None
    additional_fields: Optional[dict] = None


@dataclass
class UserRecord:
    """Parsed user info from users.parquet."""

    instrument_id: str
    run_number: int
    run_id: str
    user_id: Optional[str] = None
    name: Optional[str] = None
    facility_user_id: Optional[str] = None
    role: Optional[str] = None
    additional_fields: Optional[dict] = None


@dataclass
class DASLogRecord:
    """Single DAS log entry from daslogs.parquet."""

    instrument_id: str
    run_number: int
    run_id: str
    log_name: str
    device_name: Optional[str] = None
    device_id: Optional[str] = None
    time: Optional[float] = None
    value: Optional[str] = None
    value_numeric: Optional[float] = None
    average_value: Optional[float] = None
    min_value: Optional[float] = None
    max_value: Optional[float] = None


@dataclass
class ParquetData:
    """
    Complete parsed data from nexus-processor parquet files.

    Contains all data extracted from a set of parquet files
    for a single run.
    """

    metadata: Optional[MetadataRecord] = None
    sample: Optional[SampleRecord] = None
    instrument: Optional[InstrumentRecord] = None
    users: list[UserRecord] = field(default_factory=list)
    daslogs: dict[str, DASLogRecord] = field(default_factory=dict)

    @property
    def run_number(self) -> Optional[int]:
        """Get run number from metadata."""
        if self.metadata:
            return self.metadata.run_number
        return None

    @property
    def instrument_id(self) -> Optional[str]:
        """Get instrument ID from metadata."""
        if self.metadata:
            return self.metadata.instrument_id
        return None

    @property
    def experiment_identifier(self) -> Optional[str]:
        """Get experiment identifier (IPTS) from metadata."""
        if self.metadata:
            return self.metadata.experiment_identifier
        return None

    def get_daslog_summary(self, log_name: str) -> Optional[dict]:
        """
        Get summary statistics for a DAS log.

        Returns dict with average_value, min_value, max_value.
        """
        if log_name in self.daslogs:
            log = self.daslogs[log_name]
            return {
                "average_value": log.average_value,
                "min_value": log.min_value,
                "max_value": log.max_value,
            }
        return None


class ParquetParser:
    """
    Parser for nexus-processor parquet output files.

    Reads parquet files from a directory and assembles them
    into a ParquetData structure.

    Usage:
        parser = ParquetParser()
        data = parser.parse_directory("/path/to/parquet_output")

        # Or parse individual files
        metadata = parser.parse_metadata("/path/to/metadata.parquet")
    """

    def __init__(self):
        """Initialize the parser."""
        pass

    def parse_directory(
        self,
        directory: str | Path,
        run_number: Optional[int] = None,
    ) -> ParquetData:
        """
        Parse all parquet files in a directory.

        Args:
            directory: Path to directory containing parquet files
            run_number: Optional run number to filter by

        Returns:
            ParquetData with all parsed data
        """
        directory = Path(directory)

        if not directory.exists():
            raise FileNotFoundError(f"Directory not found: {directory}")

        result = ParquetData()

        # Find and parse each file type
        for parquet_file in directory.glob("*.parquet"):
            name = parquet_file.stem.lower()

            if "metadata" in name:
                result.metadata = self.parse_metadata(parquet_file, run_number)
            elif "sample" in name and "event" not in name:
                result.sample = self.parse_sample(parquet_file, run_number)
            elif "instrument" in name:
                result.instrument = self.parse_instrument(parquet_file, run_number)
            elif "users" in name:
                result.users = self.parse_users(parquet_file, run_number)
            elif "daslogs" in name:
                result.daslogs = self.parse_daslogs(parquet_file, run_number)

        return result

    def parse_metadata(
        self,
        file_path: str | Path,
        run_number: Optional[int] = None,
    ) -> Optional[MetadataRecord]:
        """
        Parse metadata.parquet file.

        Args:
            file_path: Path to metadata.parquet
            run_number: Optional run number to filter by

        Returns:
            MetadataRecord or None if not found
        """
        table = pq.read_table(file_path)
        df = table.to_pandas()

        if run_number is not None:
            df = df[df["run_number"] == run_number]

        if df.empty:
            return None

        row = df.iloc[0]

        return MetadataRecord(
            instrument_id=self._get_value(row, "instrument_id"),
            run_number=int(self._get_value(row, "run_number")),
            run_id=self._get_value(row, "run_id"),
            title=self._get_value(row, "title"),
            start_time=self._get_value(row, "start_time"),
            end_time=self._get_value(row, "end_time"),
            duration=self._get_value(row, "duration"),
            proton_charge=self._get_value(row, "proton_charge"),
            total_counts=self._get_value(row, "total_counts"),
            experiment_identifier=self._get_value(row, "experiment_identifier"),
            definition=self._get_value(row, "definition"),
            source_file=self._get_value(row, "source_file"),
            source_path=self._get_value(row, "source_path"),
            ingestion_time=self._get_value(row, "ingestion_time"),
            file_attributes=self._get_dict_value(row, "file_attributes"),
            entry_attributes=self._get_dict_value(row, "entry_attributes"),
        )

    def parse_sample(
        self,
        file_path: str | Path,
        run_number: Optional[int] = None,
    ) -> Optional[SampleRecord]:
        """Parse sample.parquet file."""
        table = pq.read_table(file_path)
        df = table.to_pandas()

        if run_number is not None:
            df = df[df["run_number"] == run_number]

        if df.empty:
            return None

        row = df.iloc[0]

        return SampleRecord(
            instrument_id=self._get_value(row, "instrument_id"),
            run_number=int(self._get_value(row, "run_number")),
            run_id=self._get_value(row, "run_id"),
            name=self._get_value(row, "name"),
            nature=self._get_value(row, "nature"),
            chemical_formula=self._get_value(row, "chemical_formula"),
            mass=self._get_value(row, "mass"),
            temperature=self._get_value(row, "temperature"),
            additional_fields=self._get_dict_value(row, "additional_fields"),
        )

    def parse_instrument(
        self,
        file_path: str | Path,
        run_number: Optional[int] = None,
    ) -> Optional[InstrumentRecord]:
        """Parse instrument.parquet file."""
        table = pq.read_table(file_path)
        df = table.to_pandas()

        if run_number is not None:
            df = df[df["run_number"] == run_number]

        if df.empty:
            return None

        row = df.iloc[0]

        return InstrumentRecord(
            instrument_id=self._get_value(row, "instrument_id"),
            run_number=int(self._get_value(row, "run_number")),
            run_id=self._get_value(row, "run_id"),
            name=self._get_value(row, "name"),
            beamline=self._get_value(row, "beamline"),
            instrument_xml_data=self._get_value(row, "instrument_xml_data"),
            additional_fields=self._get_dict_value(row, "additional_fields"),
        )

    def parse_users(
        self,
        file_path: str | Path,
        run_number: Optional[int] = None,
    ) -> list[UserRecord]:
        """Parse users.parquet file."""
        table = pq.read_table(file_path)
        df = table.to_pandas()

        if run_number is not None:
            df = df[df["run_number"] == run_number]

        users = []
        for _, row in df.iterrows():
            users.append(
                UserRecord(
                    instrument_id=self._get_value(row, "instrument_id"),
                    run_number=int(self._get_value(row, "run_number")),
                    run_id=self._get_value(row, "run_id"),
                    user_id=self._get_value(row, "user_id"),
                    name=self._get_value(row, "name"),
                    facility_user_id=self._get_value(row, "facility_user_id"),
                    role=self._get_value(row, "role"),
                    additional_fields=self._get_dict_value(row, "additional_fields"),
                )
            )

        return users

    def parse_daslogs(
        self,
        file_path: str | Path,
        run_number: Optional[int] = None,
    ) -> dict[str, DASLogRecord]:
        """
        Parse daslogs.parquet file.

        Returns a dictionary mapping log_name to the aggregated log record
        (uses the first record for each log_name, which contains averages).
        """
        table = pq.read_table(file_path)
        df = table.to_pandas()

        if run_number is not None:
            df = df[df["run_number"] == run_number]

        # Group by log_name and take first record (contains aggregates)
        daslogs = {}
        for log_name, group in df.groupby("log_name"):
            row = group.iloc[0]
            daslogs[log_name] = DASLogRecord(
                instrument_id=self._get_value(row, "instrument_id"),
                run_number=int(self._get_value(row, "run_number")),
                run_id=self._get_value(row, "run_id"),
                log_name=log_name,
                device_name=self._get_value(row, "device_name"),
                device_id=self._get_value(row, "device_id"),
                time=self._get_value(row, "time"),
                value=self._get_value(row, "value"),
                value_numeric=self._get_value(row, "value_numeric"),
                average_value=self._get_value(row, "average_value"),
                min_value=self._get_value(row, "min_value"),
                max_value=self._get_value(row, "max_value"),
            )

        return daslogs

    def _get_value(self, row: Any, column: str) -> Any:
        """Safely get a value from a DataFrame row."""
        if column not in row.index:
            return None
        value = row[column]
        # Handle pandas NA values
        if value is None or (hasattr(value, "__class__") and "NA" in str(type(value))):
            return None
        # Handle numpy types
        if hasattr(value, "item"):
            return value.item()
        return value

    def _get_dict_value(self, row: Any, column: str) -> Optional[dict]:
        """Safely get a dict value from a DataFrame row."""
        value = self._get_value(row, column)
        if value is None:
            return None
        if isinstance(value, dict):
            return value
        # Handle pyarrow map types
        if hasattr(value, "as_py"):
            return value.as_py()
        return None


def find_parquet_files(
    base_dir: str | Path,
    run_number: int,
    instrument_id: Optional[str] = None,
) -> Optional[Path]:
    """
    Find parquet output directory for a given run.

    Searches for directories containing parquet files that match
    the given run number.

    Args:
        base_dir: Base directory to search
        run_number: Run number to find
        instrument_id: Optional instrument ID to narrow search

    Returns:
        Path to directory containing parquet files, or None if not found
    """
    base_dir = Path(base_dir)

    # Common patterns for parquet output directories
    patterns = [
        f"*{run_number}*",
        f"*_{run_number}_*",
        f"{instrument_id}_{run_number}" if instrument_id else None,
    ]

    for pattern in patterns:
        if pattern is None:
            continue
        for match in base_dir.glob(pattern):
            if match.is_dir():
                # Check if it contains parquet files
                if list(match.glob("*.parquet")):
                    return match

    # Also check if base_dir itself contains the files
    parquet_files = list(base_dir.glob("*.parquet"))
    if parquet_files:
        # Check if any file matches the run number
        for pf in parquet_files:
            if str(run_number) in pf.stem:
                return base_dir

    return None
